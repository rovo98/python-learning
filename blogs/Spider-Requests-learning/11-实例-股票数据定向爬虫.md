### “股票数据定向爬虫”实例

#### 功能描述

- 目标：获取上交所和深交所所有股票的名称和交易信息
- 输出：保存到文件中


- 技术路线：requests-re-BeautifulSoup
- 网站选择：
  - 新浪股票：http://finance.sina.com.cn/stock/
  - 百度股票：http://gupiao.baidu.com/stock/
- 候选网站的选取原则：
  - 股票信息静态存在于HTML页面中，非js代码生成，没有Robots协议限制

#### 程序结构设计

- 步骤1：从东方财富网获取股票列表
- 步骤2：根据股票列表逐个到百度股票获取个股信息
- 步骤3：将获取的信息写入到文件中

```python
import re
import requests
from bs4 import BeautifulSoup
import traceback

def getHTMLText(url):
  try:
    kv = {'user-agent':'Mozilla/5.0'}
    r = requests.get(url, timeout=30, headers=kv)
    r.raise_for_status()
    r.encoding = r.apparent_encoding
    return r.text
  except:
    print('网页获取失败')
    return ""

# 获取所有的股票信息
def getStockList(lst, stockURL):
  html = getHTMLText(stockURL)
  soup = BeautifulSoup(html, 'html.parser')
  a = soup.find_all('a')
  for i in a:
    try:
      href = i.attrs['href']
      res = re.findall(r'[s][hz]\d{6}', href)[0]
      lst.append(res)
    except:
      continue
  print('共有'+str(len(lst))+'支股票')
# 获取每一只个股的信息，并写入文本中
def getStockInfo(lst, stockURL, fpath):
  count = 0
  for stock in lst:
    url = stockURL + stock + '.html'
    html = getHTMLText(url)
    try:
      if html == "":
        continue
      infoDict = {}
      soup = BeautifulSoup(html, 'html.parser')
      stockInfo = soup.find('div',attrs={'class':'stock-bets'})
      if stockInfo is None:
        continue
      count += 1
      print("正在爬取第"+str(count)+"支股!")
      name = stockInfo.find(attrs={'class':'bets-name'})
      infoDict.update({'股票名称':name.text.split()[0]})
      keyList = stockInfo.find_all('dt')
      valueList = stockInfo.find_all('dd')
      for i  in range(len(keyList)):
        key = keyList[i].text
        value = valueList[i].text
        infoDict[key] = value

      with open(fpath, 'a', encoding='utf-8')as f :
        f.write(str(infoDict)+ '\n')
    except:
      traceback.print_exc()
def main():
  stock_list_url = 'http://quote.eastmoney.com/stocklist.html'
  stock_info_url = 'https://gupiao.baidu.com/stock/'
  output_file = 'E://BaiduStockInfo.txt'
  slist = []
  getStockList(slist, stock_list_url)
  getStockInfo(slist, stock_info_url, output_file)

if __name__ == "__main__":
  main()
  print('完成')
```

